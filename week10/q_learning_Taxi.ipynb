{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q-learning-Taxi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRyizSSW2LDA"
      },
      "source": [
        "This notebook is based on coursera's Practical RL course by National Research University Higher School of Economics, https://www.coursera.org/learn/practical-rl/home/welcome\n",
        "\n",
        "We will make a Q-Learning agent to solve OpenAI Gym's Taxi problem.\n",
        "\n",
        "Q-Learning update equation:\n",
        "\n",
        "\n",
        "*   tabular: $$ Q(s,a) := (1 - \\alpha) \\cdot Q(s,a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot V(s')) \\\\ = Q(s,a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot V(s') - Q(s,a))$$\n",
        "\n",
        "For more definitions, see also https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdVjjTaIlms4"
      },
      "source": [
        "from collections import defaultdict\n",
        "import random, math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZQyTVzLTJva"
      },
      "source": [
        "We pick a simple test environment for q-learning: picking up and dropping off customers.\n",
        "\n",
        "Note that for the Taxi environment your reward can be negative, see link for details: https://gym.openai.com/envs/Taxi-v3/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRp87KGKu8Ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2317e3c-9954-4422-8668-2b227a4746de"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "env.reset()\n",
        "\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print('Number of discrete actions:', n_actions, ' - pick up, drop off, up, down, left, right')\n",
        "# observations\n",
        "print(env.observation_space)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of discrete actions: 6  - pick up, drop off, up, down, left, right\n",
            "Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AruLHu_TYi4"
      },
      "source": [
        "Let's play a game (pick-up and drop off a customer) and see what happens if actions are just randomly sampled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5JI_2WR1cSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdc3e056-ce2e-4e35-cbe6-3579a7cd9493"
      },
      "source": [
        "# render the observations to see how your taxi navigates through the maze\n",
        "s = env.reset()\n",
        "for _ in range(30):\n",
        "    # sample an action\n",
        "    a = env.action_space.sample()\n",
        "    # get the reward and the next state and whether the game has finished\n",
        "    next_s, r, done, _ = env.step(a)\n",
        "    # set the state to the next one\n",
        "    s = next_s\n",
        "    \n",
        "    print('reward: ',r)\n",
        "    env.render()\n",
        "    \n",
        "    if done: break"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : |\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | :\u001b[43m \u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (West)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (East)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "reward:  -10\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "  (Pickup)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fprOgg9ZTnd4"
      },
      "source": [
        "Let's now build an agent which will learn a better policy to pick up customers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1nsw7Ex0Kuf"
      },
      "source": [
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
        "        \"\"\"\n",
        "        Q-Learning Agent\n",
        "        based on http://ai.berkeley.edu/projects/release/reinforcement/v1/001/docs/qlearningAgents.html\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.get_legal_actions = get_legal_actions\n",
        "        # dictionary of expected rewards for (state,action) pairs\n",
        "        # it is a dictionary of dictionaries, because for every state,\n",
        "        # there could be multiple actions, each of which has qvalue\n",
        "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "        # learning rate\n",
        "        self.alpha = alpha\n",
        "        # exploration-exploitation trade-off\n",
        "        self.epsilon = epsilon\n",
        "        # gamma - the future reward discount factor\n",
        "        self.discount = discount\n",
        "\n",
        "    def get_qvalue(self, state, action):\n",
        "        \"\"\" Returns Q(state,action) \"\"\"\n",
        "        return self._qvalues[state][action]\n",
        "\n",
        "    def set_qvalue(self,state,action,value):\n",
        "        \"\"\" Sets the Qvalue for [state,action] to the given value \"\"\"\n",
        "        self._qvalues[state][action] = value\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Compute your agent's estimate of V(s) using current q-values\n",
        "        V(s) = max_over_action Q(state,action) over possible actions.\n",
        "        Note: please take into account that q-values can be negative.\n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "        \n",
        "        # go through all possible actions, check the qvalue and return the max\n",
        "        value = float('-inf')\n",
        "        for action in possible_actions:\n",
        "          qval = self.get_qvalue(state, action)\n",
        "          if qval > value:\n",
        "            value = qval\n",
        "\n",
        "        return value\n",
        "\n",
        "    def get_best_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the best action to take in a state (using current q-values). \n",
        "        \"\"\"\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "\n",
        "        #If there are no legal actions, return 0.0\n",
        "        if len(possible_actions) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # analog to the get_value method, with a different return\n",
        "        value = float('-inf')\n",
        "        for action in possible_actions:\n",
        "          qval = self.get_qvalue(state, action)\n",
        "          if qval > value:\n",
        "            value = qval\n",
        "            best_action = action\n",
        "                                                                                  \n",
        "        return best_action\n",
        "        \n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        Compute the action to take in the current state, including exploration.  \n",
        "        With probability self.epsilon, we should take a random action.\n",
        "            otherwise - the best policy action.\n",
        "        \n",
        "        Note: To pick randomly from a list, use random.choice(list). \n",
        "              To pick True or False with a given probablity, generate uniform number in [0, 1]\n",
        "              and compare it with your probability.\n",
        "        \"\"\"\n",
        "\n",
        "        # Pick Action\n",
        "        possible_actions = self.get_legal_actions(state)\n",
        "        action = None\n",
        "\n",
        "        # If there are no legal actions, return None\n",
        "        if len(possible_actions) == 0:\n",
        "            return None\n",
        "\n",
        "        if np.random.rand() < self.epsilon:\n",
        "          chosen_action = np.random.choice(possible_actions)\n",
        "        else:\n",
        "          chosen_action = self.get_best_action(state)\n",
        "\n",
        "        \n",
        "        return chosen_action\n",
        "    \n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        \"\"\"\n",
        "        You should do your Q-Value update here:\n",
        "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
        "           \n",
        "        Hint: use get_value method\n",
        "        \"\"\"\n",
        "\n",
        "        value = (1- self.alpha) * self.get_qvalue(state, action) + self.alpha * (reward + self.discount * self.get_value(next_state))\n",
        "        \n",
        "        self.set_qvalue(state, action, value)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0sBB3ZbsSqQ"
      },
      "source": [
        "# initialize a new agent\n",
        "agent = QLearningAgent(alpha=0.5, epsilon=0.25, discount=0.99,\n",
        "                       get_legal_actions = lambda s: range(n_actions))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7AysnffDtLF"
      },
      "source": [
        "# update at each train step\n",
        "def play_and_train(env,agent,t_max=10**4):\n",
        "    \"\"\"\n",
        "    This function should \n",
        "    - run a full game, actions given by agent's e-greedy policy\n",
        "    - train agent using agent.update(...) whenever it is possible\n",
        "    - return total reward\n",
        "    \"\"\"\n",
        "    total_reward = 0.0\n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        # get agent to pick action given state s, get new state and reward, then update your agent\n",
        "        a = agent.get_action(s)\n",
        "        ns, r, done, _ = env.step(a)\n",
        "        agent.update(s, a, r, ns, done)\n",
        "        \n",
        "        s = ns\n",
        "        total_reward +=r\n",
        "        if done: break\n",
        "        \n",
        "    return total_reward"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb5lHa_KZybJ"
      },
      "source": [
        "Let us visualize the reward: play a game and update the agent, dicrease exploration (since the agent has already learned something) and plot the mean reward over the last ones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJeVP7SoVguN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f8310bf3-3a6d-4650-ce6e-85f1b07d851b"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "rewards = []\n",
        "for i in range(1000):\n",
        "    rewards.append(play_and_train(env, agent))\n",
        "    # we are making epsilon smaller \n",
        "    agent.epsilon *= 0.99\n",
        "    \n",
        "    if i %100 ==0:\n",
        "        clear_output(True)\n",
        "        print('eps =', agent.epsilon, 'mean reward =', np.mean(rewards[-10:]))\n",
        "        plt.plot(rewards)\n",
        "        plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eps = 2.9191091959171894e-05 mean reward = 7.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgw17Mw2Wcya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9746c70a-133c-4cfd-fbf1-eb03bfc1abf2"
      },
      "source": [
        "# render the observations to see how your taxi navigates through the maze\n",
        "s = env.reset()\n",
        "for _ in range(30):\n",
        "    a = agent.get_best_action(s)\n",
        "        \n",
        "    next_s, r, done, _ = env.step(a)\n",
        "    \n",
        "    s = next_s\n",
        "    \n",
        "    env.render()\n",
        "    \n",
        "    if done: break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "+---------+\n",
            "|R: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : :\u001b[42m_\u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n"
          ]
        }
      ]
    }
  ]
}